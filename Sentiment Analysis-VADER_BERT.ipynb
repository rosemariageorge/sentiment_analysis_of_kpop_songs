{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "de27840b",
      "metadata": {
        "id": "de27840b"
      },
      "source": [
        "The methodology was taken from  : https://nlpiation.medium.com/is-it-possible-to-do-sentiment-analysis-on-unlabeled-data-using-bert-feat-vader-experiment-357bba53768c"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dac46ed4",
      "metadata": {
        "id": "dac46ed4"
      },
      "source": [
        "# importing and installing packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bNstBx5bdXU7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNstBx5bdXU7",
        "outputId": "1afe68dc-12d1-45c5-e459-68e98dac76bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split  # Import train_test_split function\n",
        "import nltk\n",
        "nltk.download(\"vader_lexicon\")\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import torch\n",
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments, BertTokenizer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Mh1i8VHabOMP",
      "metadata": {
        "id": "Mh1i8VHabOMP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5684193-581f-4a6c-de3c-6c96c4a73a4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wWEf5-CtYfR8",
      "metadata": {
        "id": "wWEf5-CtYfR8"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers\n",
        "!pip install --upgrade simpletransformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bcihkpWYuwm",
      "metadata": {
        "id": "2bcihkpWYuwm"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A5J7oyIUY-Ss",
      "metadata": {
        "id": "A5J7oyIUY-Ss"
      },
      "outputs": [],
      "source": [
        "pip install git+https://github.com/huggingface/transformers.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9a6e0f6",
      "metadata": {
        "id": "e9a6e0f6"
      },
      "source": [
        "# Reading the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93fb254c",
      "metadata": {
        "id": "93fb254c"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/gdrive/MyDrive/bert_result/KpopMusic_Labelled.csv\", index_col = [0])\n",
        "temp2 = df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rkhhVPLnbr5z",
      "metadata": {
        "id": "rkhhVPLnbr5z"
      },
      "outputs": [],
      "source": [
        "# Define the convert_label function\n",
        "def convert_label(inp):\n",
        "    if inp == 'Positive':\n",
        "        return 1\n",
        "    elif inp == 'Negative':\n",
        "        return 0\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Filter out rows with 'Neutral' sentiment and then apply the conversion to the remaining rows.\n",
        "temp2 = temp2[temp2[\"sentiment\"] != 'Neutral']\n",
        "temp2.loc[:, \"sentiment\"] = temp2[\"sentiment\"].apply(lambda x: convert_label(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "148906a5",
      "metadata": {
        "id": "148906a5"
      },
      "source": [
        "## Convert the sentiment label in the dataframe from {1, -1} to {1, 0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ba227a6",
      "metadata": {
        "id": "5ba227a6"
      },
      "outputs": [],
      "source": [
        "def convert_label(inp):\n",
        "    return 0 if inp == 'Negative' else 1\n",
        "\n",
        "temp2[\"sentiment\"] = temp2[\"sentiment\"].apply(lambda x: convert_label(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6e1334f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6e1334f",
        "outputId": "9358305d-a551-48fa-de66-643c6e551b2c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    637\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "temp2['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1578dfc2",
      "metadata": {
        "id": "1578dfc2"
      },
      "outputs": [],
      "source": [
        "test_df = temp2[['LyricsClean', 'sentiment']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Load the data from the CSV file\n",
        "df = pd.read_csv(\"/content/gdrive/MyDrive/bert_result/KpopMusic_Labelled.csv\", index_col=[0])\n",
        "\n",
        "# Define the convert_label function to convert sentiments to numeric values\n",
        "def convert_label(inp):\n",
        "    return 0 if inp == 'Negative' else 1\n",
        "\n",
        "# Filter out rows with 'Neutral' sentiment and then apply the conversion to the remaining rows.\n",
        "df = df[df[\"sentiment\"] != 'Neutral']\n",
        "df[\"sentiment\"] = df[\"sentiment\"].apply(lambda x: convert_label(x))\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "train, test = train_test_split(df, test_size=0.2, random_state=8)\n",
        "train, val = train_test_split(train, test_size=0.25, random_state=8)\n",
        "\n",
        "# Sentiment Analysis using VADER\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def vader_sentiment_result(sent):\n",
        "    scores = analyzer.polarity_scores(sent)\n",
        "\n",
        "    if scores[\"neg\"] > scores[\"pos\"]:\n",
        "        return 0\n",
        "\n",
        "    return 1\n",
        "\n",
        "# Convert \"LyricsClean\" column to string type\n",
        "train[\"LyricsClean\"] = train[\"LyricsClean\"].astype(str)\n",
        "val[\"LyricsClean\"] = val[\"LyricsClean\"].astype(str)\n",
        "test[\"LyricsClean\"] = test[\"LyricsClean\"].astype(str)  # Add this line to convert the test set as well\n",
        "\n",
        "# Apply sentiment analysis function\n",
        "train[\"vader_result\"] = train[\"LyricsClean\"].apply(lambda x: vader_sentiment_result(x))\n",
        "val[\"vader_result\"] = val[\"LyricsClean\"].apply(lambda x: vader_sentiment_result(x))\n",
        "test[\"vader_result\"] = test[\"LyricsClean\"].apply(lambda x: vader_sentiment_result(x))  # Add this line for the test set\n",
        "\n",
        "print(train.shape)\n",
        "print(val.shape)\n",
        "print(test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkNcx4a9ZQ5S",
        "outputId": "9f783855-283c-416a-dbff-9d2665362d45"
      },
      "id": "HkNcx4a9ZQ5S",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(381, 14)\n",
            "(128, 14)\n",
            "(128, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9545552",
      "metadata": {
        "id": "a9545552"
      },
      "source": [
        "## Split the dataset into training, validation and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "120c62f4",
      "metadata": {
        "id": "120c62f4"
      },
      "outputs": [],
      "source": [
        "train, test = train_test_split(test_df, test_size=0.2, shuffle = True, random_state = 8)\n",
        "train, val = train_test_split(train, test_size=0.25, random_state= 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2d5ea23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2d5ea23",
        "outputId": "ee2dc204-c814-4824-db29-64ff18ba2d48"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(381, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abe2502c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abe2502c",
        "outputId": "c53939b7-b834-46f0-d804-04bdbfd57a1e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d31ccfcc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d31ccfcc",
        "outputId": "b0fa992d-b85c-486f-8c4e-2689c383a1d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23602f6a",
      "metadata": {
        "id": "23602f6a"
      },
      "source": [
        "# VADER analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79200865",
      "metadata": {
        "id": "79200865"
      },
      "outputs": [],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def vader_sentiment_result(sent):\n",
        "    scores = analyzer.polarity_scores(sent)\n",
        "\n",
        "    if scores[\"neg\"] > scores[\"pos\"]:\n",
        "        return 0\n",
        "\n",
        "    return 1\n",
        "\n",
        "# Convert \"LyricsClean\" column to string type\n",
        "train[\"LyricsClean\"] = train[\"LyricsClean\"].astype(str)\n",
        "val[\"LyricsClean\"] = val[\"LyricsClean\"].astype(str)\n",
        "\n",
        "# Apply sentiment analysis function\n",
        "train[\"vader_result\"] = train[\"LyricsClean\"].apply(lambda x: vader_sentiment_result(x))\n",
        "val[\"vader_result\"] = val[\"LyricsClean\"].apply(lambda x: vader_sentiment_result(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34df4ce6",
      "metadata": {
        "id": "34df4ce6"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37826787",
      "metadata": {
        "id": "37826787"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3967d00b",
      "metadata": {
        "id": "3967d00b"
      },
      "source": [
        "Please note that training the BERT model will take a long time. Training of even 5 epochs took around 2-3 hours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Dlp2J8svd8WN",
      "metadata": {
        "id": "Dlp2J8svd8WN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e634228-79e9-46cf-b60d-82d0df65bfc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.65.0)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n",
            "Collecting accelerate>=0.20.3 (from transformers[torch])\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (16.0.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.21.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[torch]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wVbZj7GbflBr",
      "metadata": {
        "id": "wVbZj7GbflBr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "039659e9-ac6c-4873-c784-5eb64ffb3398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.8.2\n",
            "  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.8.2) (3.12.2)\n",
            "Collecting huggingface-hub==0.0.12 (from transformers==4.8.2)\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.8.2) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from transformers==4.8.2) (23.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from transformers==4.8.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.8.2) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.8.2) (2.27.1)\n",
            "Collecting sacremoses (from transformers==4.8.2)\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tokenizers<0.11,>=0.10.1 (from transformers==4.8.2)\n",
            "  Downloading tokenizers-0.10.3.tar.gz (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.7/212.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.8.2) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.0.12->transformers==4.8.2) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.8.2) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.8.2) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.8.2) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.8.2) (3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.8.2) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.8.2) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.8.2) (1.3.1)\n",
            "Building wheels for collected packages: tokenizers, sacremoses\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
            "\u001b[0m  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895239 sha256=ca4527986d5848491b6d7e78b15759572b79b190e8a81842203ba07654c82ab7\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "Successfully built sacremoses\n",
            "Failed to build tokenizers\n",
            "\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.9.0 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.9.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.8.2\n",
        "!pip install torch==1.9.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Od-PGGgjeJ2S",
      "metadata": {
        "id": "Od-PGGgjeJ2S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8faffb9-bf1b-4bf8-b198-84ce1189e50b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"USE_ACCELERATE\"] = \"0\"\n",
        "\n",
        "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "nltk.download(\"vader_lexicon\")\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import torch\n",
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments, BertTokenizer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "epoch 5"
      ],
      "metadata": {
        "id": "ctsAyQIZfirE"
      },
      "id": "ctsAyQIZfirE"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "# Initialize the tokenizer and model for BERT\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\")\n",
        "\n",
        "# The dataset class\n",
        "class TheDataset(Dataset):\n",
        "    def __init__(self, reviews, sentiments, tokenizer):\n",
        "        self.reviews = reviews\n",
        "        self.sentiments = sentiments\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = tokenizer.model_max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        review = str(self.reviews[index])\n",
        "        sentiments = self.sentiments[index]\n",
        "\n",
        "        encoded_review = self.tokenizer.encode_plus(\n",
        "            review,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            padding=\"max_length\",\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoded_review['input_ids'][0],\n",
        "            'attention_mask': encoded_review['attention_mask'][0],\n",
        "            'labels': torch.tensor(sentiments, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Prepare the Train/Validation sets\n",
        "train_dataset = TheDataset(\n",
        "    reviews=train.LyricsClean.tolist(),\n",
        "    sentiments=train.sentiment.tolist(),\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "val_dataset = TheDataset(\n",
        "    reviews=val.LyricsClean.tolist(),\n",
        "    sentiments=val.sentiment.tolist(),\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Freeze BERT except (the 24th layer + the last pooler layer)\n",
        "for name, param in model.bert.named_parameters():\n",
        "    if (not name.startswith('pooler')) and \"layer.23\" not in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Define the function to get the accuracy\n",
        "def compute_metrics(preds, labels):\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Define the training parameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "train_batch_size = 16\n",
        "eval_batch_size = 64\n",
        "num_train_epochs = 5\n",
        "weight_decay = 0.01\n",
        "warmup_steps = 500\n",
        "learning_rate = 2e-5\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=eval_batch_size)\n",
        "\n",
        "# Create optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "total_steps = len(train_dataloader) * num_train_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
        "\n",
        "# Start pre-training!\n",
        "model.train()\n",
        "for epoch in range(num_train_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_train_epochs}\")\n",
        "    total_loss = 0.0\n",
        "    total_steps = len(train_dataloader)\n",
        "\n",
        "    # Initialize variables to store metrics for the epoch\n",
        "    epoch_preds = []\n",
        "    epoch_labels = []\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Get predictions and labels to compute metrics\n",
        "        logits = outputs.logits\n",
        "        preds = logits.argmax(dim=1)\n",
        "        labels = batch['labels']\n",
        "\n",
        "        # Store predictions and labels for the epoch\n",
        "        epoch_preds.extend(preds.detach().cpu().numpy())\n",
        "        epoch_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            print(f\"Step {step}/{total_steps}, Loss: {loss.item()}\")\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    avg_loss = total_loss / total_steps\n",
        "    print(f\"Avg Loss for Epoch {epoch + 1}: {avg_loss}\")\n",
        "\n",
        "    # Calculate metrics for the epoch\n",
        "    metrics = compute_metrics(preds=epoch_preds, labels=epoch_labels)\n",
        "    print(f\"Epoch {epoch + 1} Metrics:\")\n",
        "    print(\"Accuracy: \", metrics['accuracy'])\n",
        "    print(\"Precision: \", metrics['precision'])\n",
        "    print(\"Recall: \", metrics['recall'])\n",
        "    print(\"F1 Score: \", metrics['f1'])\n",
        "\n",
        "    print(\"Epoch completed.\")\n",
        "\n",
        "    # Now, let's validate the model after each epoch\n",
        "    model.eval()\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for val_batch in val_dataloader:\n",
        "            val_batch = {k: v.to(device) for k, v in val_batch.items()}\n",
        "            val_outputs = model(**val_batch)\n",
        "            val_logits = val_outputs.logits\n",
        "            val_preds.extend(val_logits.argmax(dim=1).detach().cpu().numpy())\n",
        "            val_labels.extend(val_batch['labels'].detach().cpu().numpy())\n",
        "\n",
        "    val_metrics = compute_metrics(preds=val_preds, labels=val_labels)\n",
        "    print(\"Validation Metrics:\")\n",
        "    print(\"Accuracy: \", val_metrics['accuracy'])\n",
        "    print(\"Precision: \", val_metrics['precision'])\n",
        "    print(\"Recall: \", val_metrics['recall'])\n",
        "    print(\"F1 Score: \", val_metrics['f1'])\n",
        "\n",
        "    # Set the model back to training mode\n",
        "    model.train()\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(\"/content/gdrive/MyDrive/bert_result/epoch 5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gSkNSbSTxT4",
        "outputId": "fdf727d4-0f09-45cc-9061-9654dfffcdfe"
      },
      "id": "0gSkNSbSTxT4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Step 0/24, Loss: 1.080095887184143\n",
            "Avg Loss for Epoch 1: 0.9703689490755399\n",
            "Epoch 1 Metrics:\n",
            "Accuracy:  0.2230971128608924\n",
            "Precision:  0.75\n",
            "Recall:  0.02\n",
            "F1 Score:  0.03896103896103896\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.1484375\n",
            "Precision:  0.0\n",
            "Recall:  0.0\n",
            "F1 Score:  0.0\n",
            "Epoch 2/5\n",
            "Step 0/24, Loss: 0.9945811033248901\n",
            "Avg Loss for Epoch 2: 0.9410428057114283\n",
            "Epoch 2 Metrics:\n",
            "Accuracy:  0.23097112860892388\n",
            "Precision:  0.7333333333333333\n",
            "Recall:  0.03666666666666667\n",
            "F1 Score:  0.06984126984126984\n",
            "Epoch completed.\n",
            "Validation Metrics:\n",
            "Accuracy:  0.15625\n",
            "Precision:  1.0\n",
            "Recall:  0.009174311926605505\n",
            "F1 Score:  0.018181818181818184\n",
            "Epoch 3/5\n",
            "Step 0/24, Loss: 0.8799847364425659\n",
            "Avg Loss for Epoch 3: 0.842610277235508\n",
            "Epoch 3 Metrics:\n",
            "Accuracy:  0.28346456692913385\n",
            "Precision:  0.7647058823529411\n",
            "Recall:  0.13\n",
            "F1 Score:  0.22222222222222224\n",
            "Epoch completed.\n",
            "Validation Metrics:\n",
            "Accuracy:  0.3203125\n",
            "Precision:  0.8928571428571429\n",
            "Recall:  0.22935779816513763\n",
            "F1 Score:  0.36496350364963503\n",
            "Epoch 4/5\n",
            "Step 0/24, Loss: 0.7563066482543945\n",
            "Avg Loss for Epoch 4: 0.7132726137836775\n",
            "Epoch 4 Metrics:\n",
            "Accuracy:  0.5170603674540682\n",
            "Precision:  0.8222222222222222\n",
            "Recall:  0.49333333333333335\n",
            "F1 Score:  0.6166666666666666\n",
            "Epoch completed.\n",
            "Validation Metrics:\n",
            "Accuracy:  0.671875\n",
            "Precision:  0.8316831683168316\n",
            "Recall:  0.7706422018348624\n",
            "F1 Score:  0.7999999999999999\n",
            "Epoch 5/5\n",
            "Step 0/24, Loss: 0.6964261531829834\n",
            "Avg Loss for Epoch 5: 0.6105707076688608\n",
            "Epoch 5 Metrics:\n",
            "Accuracy:  0.6797900262467191\n",
            "Precision:  0.788961038961039\n",
            "Recall:  0.81\n",
            "F1 Score:  0.799342105263158\n",
            "Epoch completed.\n",
            "Validation Metrics:\n",
            "Accuracy:  0.828125\n",
            "Precision:  0.848\n",
            "Recall:  0.9724770642201835\n",
            "F1 Score:  0.9059829059829061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "epoch 10"
      ],
      "metadata": {
        "id": "Cl8tNg-Qfk5l"
      },
      "id": "Cl8tNg-Qfk5l"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "# Initialize the tokenizer and model for BERT\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\")\n",
        "\n",
        "# The dataset class\n",
        "class TheDataset(Dataset):\n",
        "    def __init__(self, reviews, sentiments, tokenizer):\n",
        "        self.reviews = reviews\n",
        "        self.sentiments = sentiments\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = tokenizer.model_max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        review = str(self.reviews[index])\n",
        "        sentiments = self.sentiments[index]\n",
        "\n",
        "        encoded_review = self.tokenizer.encode_plus(\n",
        "            review,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            padding=\"max_length\",\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoded_review['input_ids'][0],\n",
        "            'attention_mask': encoded_review['attention_mask'][0],\n",
        "            'labels': torch.tensor(sentiments, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Prepare the Train/Validation sets\n",
        "train_dataset = TheDataset(\n",
        "    reviews=train.LyricsClean.tolist(),\n",
        "    sentiments=train.sentiment.tolist(),\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "val_dataset = TheDataset(\n",
        "    reviews=val.LyricsClean.tolist(),\n",
        "    sentiments=val.sentiment.tolist(),\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Freeze BERT except (the 24th layer + the last pooler layer)\n",
        "for name, param in model.bert.named_parameters():\n",
        "    if (not name.startswith('pooler')) and \"layer.23\" not in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Define the function to get the accuracy\n",
        "def compute_metrics(preds, labels):\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Define the training parameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "train_batch_size = 16\n",
        "eval_batch_size = 64\n",
        "num_train_epochs = 10\n",
        "weight_decay = 0.01\n",
        "warmup_steps = 500\n",
        "learning_rate = 2e-5\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=eval_batch_size)\n",
        "\n",
        "# Create optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "total_steps = len(train_dataloader) * num_train_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
        "\n",
        "# Start pre-training!\n",
        "model.train()\n",
        "for epoch in range(num_train_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_train_epochs}\")\n",
        "    total_loss = 0.0\n",
        "    total_steps = len(train_dataloader)\n",
        "\n",
        "    # Initialize variables to store metrics for the epoch\n",
        "    epoch_preds = []\n",
        "    epoch_labels = []\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Get predictions and labels to compute metrics\n",
        "        logits = outputs.logits\n",
        "        preds = logits.argmax(dim=1)\n",
        "        labels = batch['labels']\n",
        "\n",
        "        # Store predictions and labels for the epoch\n",
        "        epoch_preds.extend(preds.detach().cpu().numpy())\n",
        "        epoch_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            print(f\"Step {step}/{total_steps}, Loss: {loss.item()}\")\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    avg_loss = total_loss / total_steps\n",
        "    print(f\"Avg Loss for Epoch {epoch + 1}: {avg_loss}\")\n",
        "\n",
        "    # Calculate metrics for the epoch\n",
        "    metrics = compute_metrics(preds=epoch_preds, labels=epoch_labels)\n",
        "    print(f\"Epoch {epoch + 1} Metrics:\")\n",
        "    print(\"Accuracy: \", metrics['accuracy'])\n",
        "    print(\"Precision: \", metrics['precision'])\n",
        "    print(\"Recall: \", metrics['recall'])\n",
        "    print(\"F1 Score: \", metrics['f1'])\n",
        "\n",
        "    print(\"Epoch completed.\")\n",
        "\n",
        "    # Now, let's validate the model after each epoch\n",
        "    model.eval()\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for val_batch in val_dataloader:\n",
        "            val_batch = {k: v.to(device) for k, v in val_batch.items()}\n",
        "            val_outputs = model(**val_batch)\n",
        "            val_logits = val_outputs.logits\n",
        "            val_preds.extend(val_logits.argmax(dim=1).detach().cpu().numpy())\n",
        "            val_labels.extend(val_batch['labels'].detach().cpu().numpy())\n",
        "\n",
        "    val_metrics = compute_metrics(preds=val_preds, labels=val_labels)\n",
        "    print(\"Validation Metrics:\")\n",
        "    print(\"Accuracy: \", val_metrics['accuracy'])\n",
        "    print(\"Precision: \", val_metrics['precision'])\n",
        "    print(\"Recall: \", val_metrics['recall'])\n",
        "    print(\"F1 Score: \", val_metrics['f1'])\n",
        "\n",
        "    # Set the model back to training mode\n",
        "    model.train()\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(\"/content/gdrive/MyDrive/bert_result/epoch_10\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6gur2AdsoH6",
        "outputId": "4890f19b-0856-464a-923e-509e71fa7aaa"
      },
      "id": "f6gur2AdsoH6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Step 0/24, Loss: 0.8599921464920044\n",
            "Avg Loss for Epoch 1: 0.7765697066982588\n",
            "Epoch 1 Metrics:\n",
            "Accuracy:  0.34120734908136485\n",
            "Precision:  0.618252881433299\n",
            "Recall:  0.34120734908136485\n",
            "F1 Score:  0.362531480950502\n",
            "Epoch completed.\n",
            "Validation Metrics:\n",
            "Accuracy:  0.2578125\n",
            "Precision:  0.6941356169871794\n",
            "Recall:  0.2578125\n",
            "F1 Score:  0.27709422183507554\n",
            "Epoch 2/10\n",
            "Step 0/24, Loss: 0.8444575667381287\n",
            "Avg Loss for Epoch 2: 0.7549998884399732\n",
            "Epoch 2 Metrics:\n",
            "Accuracy:  0.3963254593175853\n",
            "Precision:  0.615161741145993\n",
            "Recall:  0.3963254593175853\n",
            "F1 Score:  0.4408271081499428\n",
            "Epoch completed.\n",
            "Validation Metrics:\n",
            "Accuracy:  0.71875\n",
            "Precision:  0.7797327070169605\n",
            "Recall:  0.71875\n",
            "F1 Score:  0.7443082524271845\n",
            "Epoch 3/10\n",
            "Step 0/24, Loss: 0.6408136487007141\n",
            "Avg Loss for Epoch 3: 0.6720280547936758\n",
            "Epoch 3 Metrics:\n",
            "Accuracy:  0.5905511811023622\n",
            "Precision:  0.6576771653543306\n",
            "Recall:  0.5905511811023622\n",
            "F1 Score:  0.6180775126095897\n",
            "Epoch completed.\n",
            "Validation Metrics:\n",
            "Accuracy:  0.8203125\n",
            "Precision:  0.7506617144808743\n",
            "Recall:  0.8203125\n",
            "F1 Score:  0.7786498917748916\n",
            "Epoch 4/10\n",
            "Step 0/24, Loss: 0.6910978555679321\n",
            "Avg Loss for Epoch 4: 0.6024011870225271\n",
            "Epoch 4 Metrics:\n",
            "Accuracy:  0.7086614173228346\n",
            "Precision:  0.6435688901719144\n",
            "Recall:  0.7086614173228346\n",
            "F1 Score:  0.6713642072652289\n",
            "Epoch completed.\n",
            "Validation Metrics:\n",
            "Accuracy:  0.84375\n",
            "Precision:  0.7241633858267716\n",
            "Recall:  0.84375\n",
            "F1 Score:  0.779396186440678\n",
            "Epoch 5/10\n",
            "Step 0/24, Loss: 0.6033622026443481\n",
            "Avg Loss for Epoch 5: 0.5480258588989576\n",
            "Epoch 5 Metrics:\n",
            "Accuracy:  0.7637795275590551\n",
            "Precision:  0.6510744476338451\n",
            "Recall:  0.7637795275590551\n",
            "F1 Score:  0.6903612777795055\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 6/10\n",
            "Step 0/24, Loss: 0.38142159581184387\n",
            "Avg Loss for Epoch 6: 0.5298837261895338\n",
            "Epoch 6 Metrics:\n",
            "Accuracy:  0.7821522309711286\n",
            "Precision:  0.6191178609270147\n",
            "Recall:  0.7821522309711286\n",
            "F1 Score:  0.6911507195621165\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 7/10\n",
            "Step 0/24, Loss: 0.40712422132492065\n",
            "Avg Loss for Epoch 7: 0.5218589132030805\n",
            "Epoch 7 Metrics:\n",
            "Accuracy:  0.7821522309711286\n",
            "Precision:  0.6191178609270147\n",
            "Recall:  0.7821522309711286\n",
            "F1 Score:  0.6911507195621165\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 8/10\n",
            "Step 0/24, Loss: 0.5508404970169067\n",
            "Avg Loss for Epoch 8: 0.5308008411278328\n",
            "Epoch 8 Metrics:\n",
            "Accuracy:  0.7874015748031497\n",
            "Precision:  0.62000124000248\n",
            "Recall:  0.7874015748031497\n",
            "F1 Score:  0.6937458808838324\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 9/10\n",
            "Step 0/24, Loss: 0.35432323813438416\n",
            "Avg Loss for Epoch 9: 0.5209954480330149\n",
            "Epoch 9 Metrics:\n",
            "Accuracy:  0.7874015748031497\n",
            "Precision:  0.62000124000248\n",
            "Recall:  0.7874015748031497\n",
            "F1 Score:  0.6937458808838324\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 10/10\n",
            "Step 0/24, Loss: 0.43572109937667847\n",
            "Avg Loss for Epoch 10: 0.5093133306751648\n",
            "Epoch 10 Metrics:\n",
            "Accuracy:  0.7874015748031497\n",
            "Precision:  0.62000124000248\n",
            "Recall:  0.7874015748031497\n",
            "F1 Score:  0.6937458808838324\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "epoch = 15"
      ],
      "metadata": {
        "id": "d61aOlOBf7Xc"
      },
      "id": "d61aOlOBf7Xc"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "# Initialize the tokenizer and model for BERT\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\")\n",
        "\n",
        "# The dataset class\n",
        "class TheDataset(Dataset):\n",
        "    def __init__(self, reviews, sentiments, tokenizer):\n",
        "        self.reviews = reviews\n",
        "        self.sentiments = sentiments\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = tokenizer.model_max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        review = str(self.reviews[index])\n",
        "        sentiments = self.sentiments[index]\n",
        "\n",
        "        encoded_review = self.tokenizer.encode_plus(\n",
        "            review,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            padding=\"max_length\",\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoded_review['input_ids'][0],\n",
        "            'attention_mask': encoded_review['attention_mask'][0],\n",
        "            'labels': torch.tensor(sentiments, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Prepare the Train/Validation sets\n",
        "train_dataset = TheDataset(\n",
        "    reviews=train.LyricsClean.tolist(),\n",
        "    sentiments=train.sentiment.tolist(),\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "val_dataset = TheDataset(\n",
        "    reviews=val.LyricsClean.tolist(),\n",
        "    sentiments=val.sentiment.tolist(),\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Freeze BERT except (the 24th layer + the last pooler layer)\n",
        "for name, param in model.bert.named_parameters():\n",
        "    if (not name.startswith('pooler')) and \"layer.23\" not in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Define the function to get the accuracy\n",
        "def compute_metrics(preds, labels):\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Define the training parameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "train_batch_size = 16\n",
        "eval_batch_size = 64\n",
        "num_train_epochs = 15\n",
        "weight_decay = 0.01\n",
        "warmup_steps = 500\n",
        "learning_rate = 2e-5\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=eval_batch_size)\n",
        "\n",
        "# Create optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "total_steps = len(train_dataloader) * num_train_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
        "\n",
        "# Start pre-training!\n",
        "model.train()\n",
        "for epoch in range(num_train_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_train_epochs}\")\n",
        "    total_loss = 0.0\n",
        "    total_steps = len(train_dataloader)\n",
        "\n",
        "    # Initialize variables to store metrics for the epoch\n",
        "    epoch_preds = []\n",
        "    epoch_labels = []\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Get predictions and labels to compute metrics\n",
        "        logits = outputs.logits\n",
        "        preds = logits.argmax(dim=1)\n",
        "        labels = batch['labels']\n",
        "\n",
        "        # Store predictions and labels for the epoch\n",
        "        epoch_preds.extend(preds.detach().cpu().numpy())\n",
        "        epoch_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            print(f\"Step {step}/{total_steps}, Loss: {loss.item()}\")\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    avg_loss = total_loss / total_steps\n",
        "    print(f\"Avg Loss for Epoch {epoch + 1}: {avg_loss}\")\n",
        "\n",
        "    # Calculate metrics for the epoch\n",
        "    metrics = compute_metrics(preds=epoch_preds, labels=epoch_labels)\n",
        "    print(f\"Epoch {epoch + 1} Metrics:\")\n",
        "    print(\"Accuracy: \", metrics['accuracy'])\n",
        "    print(\"Precision: \", metrics['precision'])\n",
        "    print(\"Recall: \", metrics['recall'])\n",
        "    print(\"F1 Score: \", metrics['f1'])\n",
        "\n",
        "    print(\"Epoch completed.\")\n",
        "\n",
        "    # Now, let's validate the model after each epoch\n",
        "    model.eval()\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for val_batch in val_dataloader:\n",
        "            val_batch = {k: v.to(device) for k, v in val_batch.items()}\n",
        "            val_outputs = model(**val_batch)\n",
        "            val_logits = val_outputs.logits\n",
        "            val_preds.extend(val_logits.argmax(dim=1).detach().cpu().numpy())\n",
        "            val_labels.extend(val_batch['labels'].detach().cpu().numpy())\n",
        "\n",
        "    val_metrics = compute_metrics(preds=val_preds, labels=val_labels)\n",
        "    print(\"Validation Metrics:\")\n",
        "    print(\"Accuracy: \", val_metrics['accuracy'])\n",
        "    print(\"Precision: \", val_metrics['precision'])\n",
        "    print(\"Recall: \", val_metrics['recall'])\n",
        "    print(\"F1 Score: \", val_metrics['f1'])\n",
        "\n",
        "    # Set the model back to training mode\n",
        "    model.train()\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(\"/content/gdrive/MyDrive/bert_result/epoch 15\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKIQYSNMvBmR",
        "outputId": "5a52e960-686a-4007-c978-1e0262af09ca"
      },
      "id": "PKIQYSNMvBmR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "Step 0/24, Loss: 1.0479450225830078\n",
            "Avg Loss for Epoch 1: 1.1277041286230087\n",
            "Epoch 1 Metrics:\n",
            "Accuracy:  0.2152230971128609\n",
            "Precision:  0.8327186075424783\n",
            "Recall:  0.2152230971128609\n",
            "F1 Score:  0.07994111213826163\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.1484375\n",
            "Precision:  0.02203369140625\n",
            "Recall:  0.1484375\n",
            "F1 Score:  0.03837159863945579\n",
            "Epoch 2/15\n",
            "Step 0/24, Loss: 0.9533699154853821\n",
            "Avg Loss for Epoch 2: 1.0665076573689778\n",
            "Epoch 2 Metrics:\n",
            "Accuracy:  0.2178477690288714\n",
            "Precision:  0.832838177549706\n",
            "Recall:  0.2178477690288714\n",
            "F1 Score:  0.08530077923962355\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.1484375\n",
            "Precision:  0.02203369140625\n",
            "Recall:  0.1484375\n",
            "F1 Score:  0.03837159863945579\n",
            "Epoch 3/15\n",
            "Step 0/24, Loss: 1.0255250930786133\n",
            "Avg Loss for Epoch 3: 0.9579847231507301\n",
            "Epoch 3 Metrics:\n",
            "Accuracy:  0.2283464566929134\n",
            "Precision:  0.8333228346456693\n",
            "Recall:  0.2283464566929134\n",
            "F1 Score:  0.10640688102841635\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.1484375\n",
            "Precision:  0.02203369140625\n",
            "Recall:  0.1484375\n",
            "F1 Score:  0.03837159863945579\n",
            "Epoch 4/15\n",
            "Step 0/24, Loss: 0.9121289253234863\n",
            "Avg Loss for Epoch 4: 0.837949588894844\n",
            "Epoch 4 Metrics:\n",
            "Accuracy:  0.29133858267716534\n",
            "Precision:  0.6967895885175287\n",
            "Recall:  0.29133858267716534\n",
            "F1 Score:  0.247569129586903\n",
            "Epoch completed.\n",
            "Validation Metrics:\n",
            "Accuracy:  0.265625\n",
            "Precision:  0.7521069759679572\n",
            "Recall:  0.265625\n",
            "F1 Score:  0.2735157203907204\n",
            "Epoch 5/15\n",
            "Step 0/24, Loss: 0.8029141426086426\n",
            "Avg Loss for Epoch 5: 0.727706678211689\n",
            "Epoch 5 Metrics:\n",
            "Accuracy:  0.47506561679790027\n",
            "Precision:  0.6749578837856537\n",
            "Recall:  0.47506561679790027\n",
            "F1 Score:  0.5188788214659982\n",
            "Epoch completed.\n",
            "Validation Metrics:\n",
            "Accuracy:  0.8125\n",
            "Precision:  0.7460891381345927\n",
            "Recall:  0.8125\n",
            "F1 Score:  0.7741220735785954\n",
            "Epoch 6/15\n",
            "Step 0/24, Loss: 0.6838655471801758\n",
            "Avg Loss for Epoch 6: 0.6220186774929365\n",
            "Epoch 6 Metrics:\n",
            "Accuracy:  0.7007874015748031\n",
            "Precision:  0.6531222367257569\n",
            "Recall:  0.7007874015748031\n",
            "F1 Score:  0.6739100794218904\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 7/15\n",
            "Step 0/24, Loss: 0.660125195980072\n",
            "Avg Loss for Epoch 7: 0.5487492121756077\n",
            "Epoch 7 Metrics:\n",
            "Accuracy:  0.7742782152230971\n",
            "Precision:  0.6793901990752385\n",
            "Recall:  0.7742782152230971\n",
            "F1 Score:  0.7001972336196457\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 8/15\n",
            "Step 0/24, Loss: 0.744808554649353\n",
            "Avg Loss for Epoch 8: 0.5330550267050663\n",
            "Epoch 8 Metrics:\n",
            "Accuracy:  0.7874015748031497\n",
            "Precision:  0.7274946502399601\n",
            "Recall:  0.7874015748031497\n",
            "F1 Score:  0.6985928708979692\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 9/15\n",
            "Step 0/24, Loss: 0.4634752869606018\n",
            "Avg Loss for Epoch 9: 0.525495162854592\n",
            "Epoch 9 Metrics:\n",
            "Accuracy:  0.7900262467191601\n",
            "Precision:  0.8342312474098633\n",
            "Recall:  0.7900262467191601\n",
            "F1 Score:  0.6999514228583694\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 10/15\n",
            "Step 0/24, Loss: 0.6089167594909668\n",
            "Avg Loss for Epoch 10: 0.5165174622088671\n",
            "Epoch 10 Metrics:\n",
            "Accuracy:  0.7874015748031497\n",
            "Precision:  0.62000124000248\n",
            "Recall:  0.7874015748031497\n",
            "F1 Score:  0.6937458808838324\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 11/15\n",
            "Step 0/24, Loss: 0.39986705780029297\n",
            "Avg Loss for Epoch 11: 0.5151970411340395\n",
            "Epoch 11 Metrics:\n",
            "Accuracy:  0.7874015748031497\n",
            "Precision:  0.62000124000248\n",
            "Recall:  0.7874015748031497\n",
            "F1 Score:  0.6937458808838324\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 12/15\n",
            "Step 0/24, Loss: 0.5490034222602844\n",
            "Avg Loss for Epoch 12: 0.5338359661400318\n",
            "Epoch 12 Metrics:\n",
            "Accuracy:  0.7874015748031497\n",
            "Precision:  0.62000124000248\n",
            "Recall:  0.7874015748031497\n",
            "F1 Score:  0.6937458808838324\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 13/15\n",
            "Step 0/24, Loss: 0.3550386130809784\n",
            "Avg Loss for Epoch 13: 0.5056281139453253\n",
            "Epoch 13 Metrics:\n",
            "Accuracy:  0.7821522309711286\n",
            "Precision:  0.6191178609270147\n",
            "Recall:  0.7821522309711286\n",
            "F1 Score:  0.6911507195621165\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 14/15\n",
            "Step 0/24, Loss: 0.4537166953086853\n",
            "Avg Loss for Epoch 14: 0.49648387481768924\n",
            "Epoch 14 Metrics:\n",
            "Accuracy:  0.7874015748031497\n",
            "Precision:  0.62000124000248\n",
            "Recall:  0.7874015748031497\n",
            "F1 Score:  0.6937458808838324\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 15/15\n",
            "Step 0/24, Loss: 0.4646741449832916\n",
            "Avg Loss for Epoch 15: 0.4880116532246272\n",
            "Epoch 15 Metrics:\n",
            "Accuracy:  0.7874015748031497\n",
            "Precision:  0.62000124000248\n",
            "Recall:  0.7874015748031497\n",
            "F1 Score:  0.6937458808838324\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "epoch = 20"
      ],
      "metadata": {
        "id": "iaYJcIQhgDBV"
      },
      "id": "iaYJcIQhgDBV"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "# Initialize the tokenizer and model for BERT\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\")\n",
        "\n",
        "# The dataset class\n",
        "class TheDataset(Dataset):\n",
        "    def __init__(self, reviews, sentiments, tokenizer):\n",
        "        self.reviews = reviews\n",
        "        self.sentiments = sentiments\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = tokenizer.model_max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        review = str(self.reviews[index])\n",
        "        sentiments = self.sentiments[index]\n",
        "\n",
        "        encoded_review = self.tokenizer.encode_plus(\n",
        "            review,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            padding=\"max_length\",\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoded_review['input_ids'][0],\n",
        "            'attention_mask': encoded_review['attention_mask'][0],\n",
        "            'labels': torch.tensor(sentiments, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Prepare the Train/Validation sets\n",
        "train_dataset = TheDataset(\n",
        "    reviews=train.LyricsClean.tolist(),\n",
        "    sentiments=train.sentiment.tolist(),\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "val_dataset = TheDataset(\n",
        "    reviews=val.LyricsClean.tolist(),\n",
        "    sentiments=val.sentiment.tolist(),\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Freeze BERT except (the 24th layer + the last pooler layer)\n",
        "for name, param in model.bert.named_parameters():\n",
        "    if (not name.startswith('pooler')) and \"layer.23\" not in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Define the function to get the accuracy\n",
        "def compute_metrics(preds, labels):\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Define the training parameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "train_batch_size = 16\n",
        "eval_batch_size = 64\n",
        "num_train_epochs = 20\n",
        "weight_decay = 0.01\n",
        "warmup_steps = 500\n",
        "learning_rate = 2e-5\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=eval_batch_size)\n",
        "\n",
        "# Create optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "total_steps = len(train_dataloader) * num_train_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
        "\n",
        "# Start pre-training!\n",
        "model.train()\n",
        "for epoch in range(num_train_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_train_epochs}\")\n",
        "    total_loss = 0.0\n",
        "    total_steps = len(train_dataloader)\n",
        "\n",
        "    # Initialize variables to store metrics for the epoch\n",
        "    epoch_preds = []\n",
        "    epoch_labels = []\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Get predictions and labels to compute metrics\n",
        "        logits = outputs.logits\n",
        "        preds = logits.argmax(dim=1)\n",
        "        labels = batch['labels']\n",
        "\n",
        "        # Store predictions and labels for the epoch\n",
        "        epoch_preds.extend(preds.detach().cpu().numpy())\n",
        "        epoch_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            print(f\"Step {step}/{total_steps}, Loss: {loss.item()}\")\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    avg_loss = total_loss / total_steps\n",
        "    print(f\"Avg Loss for Epoch {epoch + 1}: {avg_loss}\")\n",
        "\n",
        "    # Calculate metrics for the epoch\n",
        "    metrics = compute_metrics(preds=epoch_preds, labels=epoch_labels)\n",
        "    print(f\"Epoch {epoch + 1} Metrics:\")\n",
        "    print(\"Accuracy: \", metrics['accuracy'])\n",
        "    print(\"Precision: \", metrics['precision'])\n",
        "    print(\"Recall: \", metrics['recall'])\n",
        "    print(\"F1 Score: \", metrics['f1'])\n",
        "\n",
        "    print(\"Epoch completed.\")\n",
        "\n",
        "    # Now, let's validate the model after each epoch\n",
        "    model.eval()\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for val_batch in val_dataloader:\n",
        "            val_batch = {k: v.to(device) for k, v in val_batch.items()}\n",
        "            val_outputs = model(**val_batch)\n",
        "            val_logits = val_outputs.logits\n",
        "            val_preds.extend(val_logits.argmax(dim=1).detach().cpu().numpy())\n",
        "            val_labels.extend(val_batch['labels'].detach().cpu().numpy())\n",
        "\n",
        "    val_metrics = compute_metrics(preds=val_preds, labels=val_labels)\n",
        "    print(\"Validation Metrics:\")\n",
        "    print(\"Accuracy: \", val_metrics['accuracy'])\n",
        "    print(\"Precision: \", val_metrics['precision'])\n",
        "    print(\"Recall: \", val_metrics['recall'])\n",
        "    print(\"F1 Score: \", val_metrics['f1'])\n",
        "\n",
        "    # Set the model back to training mode\n",
        "    model.train()\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(\"/content/gdrive/MyDrive/bert_result/epoch 20\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "620a9918-bf7f-4a8f-9880-9ae1645f03f2",
        "id": "IGjVUpOPgFyW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "Step 0/24, Loss: 0.7759166955947876\n",
            "Avg Loss for Epoch 1: 0.762155070900917\n",
            "Epoch 1 Metrics:\n",
            "Accuracy:  0.3700787401574803\n",
            "Precision:  0.7173814940456054\n",
            "Recall:  0.3700787401574803\n",
            "F1 Score:  0.3747227688480292\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.1484375\n",
            "Precision:  0.02203369140625\n",
            "Recall:  0.1484375\n",
            "F1 Score:  0.03837159863945579\n",
            "Epoch 2/20\n",
            "Step 0/24, Loss: 0.7590773105621338\n",
            "Avg Loss for Epoch 2: 0.7439396927754084\n",
            "Epoch 2 Metrics:\n",
            "Accuracy:  0.42782152230971127\n",
            "Precision:  0.6779376317456116\n",
            "Recall:  0.42782152230971127\n",
            "F1 Score:  0.46413216752813874\n",
            "Epoch completed.\n",
            "Validation Metrics:\n",
            "Accuracy:  0.21875\n",
            "Precision:  0.7969842657342657\n",
            "Recall:  0.21875\n",
            "F1 Score:  0.18121936274509803\n",
            "Epoch 3/20\n",
            "Step 0/24, Loss: 0.6922047734260559\n",
            "Avg Loss for Epoch 3: 0.6911280304193497\n",
            "Epoch 3 Metrics:\n",
            "Accuracy:  0.5459317585301837\n",
            "Precision:  0.6707834809397842\n",
            "Recall:  0.5459317585301837\n",
            "F1 Score:  0.5863969407803025\n",
            "Epoch completed.\n",
            "Validation Metrics:\n",
            "Accuracy:  0.8203125\n",
            "Precision:  0.7210811491935484\n",
            "Recall:  0.8203125\n",
            "F1 Score:  0.7675026824034334\n",
            "Epoch 4/20\n",
            "Step 0/24, Loss: 0.6945123076438904\n",
            "Avg Loss for Epoch 4: 0.6272533958156904\n",
            "Epoch 4 Metrics:\n",
            "Accuracy:  0.6824146981627297\n",
            "Precision:  0.6690110338149479\n",
            "Recall:  0.6824146981627297\n",
            "F1 Score:  0.6754206434134882\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 5/20\n",
            "Step 0/24, Loss: 0.5808162093162537\n",
            "Avg Loss for Epoch 5: 0.5701182248691717\n",
            "Epoch 5 Metrics:\n",
            "Accuracy:  0.7742782152230971\n",
            "Precision:  0.6674286681906697\n",
            "Recall:  0.7742782152230971\n",
            "F1 Score:  0.6960817397825272\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 6/20\n",
            "Step 0/24, Loss: 0.5442813634872437\n",
            "Avg Loss for Epoch 6: 0.5379863418638706\n",
            "Epoch 6 Metrics:\n",
            "Accuracy:  0.7926509186351706\n",
            "Precision:  0.8358714395529054\n",
            "Recall:  0.7926509186351706\n",
            "F1 Score:  0.7060350222338218\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 7/20\n",
            "Step 0/24, Loss: 0.47404801845550537\n",
            "Avg Loss for Epoch 7: 0.5497575961053371\n",
            "Epoch 7 Metrics:\n",
            "Accuracy:  0.7847769028871391\n",
            "Precision:  0.6195607128056362\n",
            "Recall:  0.7847769028871391\n",
            "F1 Score:  0.6924502084298287\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 8/20\n",
            "Step 0/24, Loss: 0.5393189787864685\n",
            "Avg Loss for Epoch 8: 0.5258138664066792\n",
            "Epoch 8 Metrics:\n",
            "Accuracy:  0.7874015748031497\n",
            "Precision:  0.62000124000248\n",
            "Recall:  0.7874015748031497\n",
            "F1 Score:  0.6937458808838324\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 9/20\n",
            "Step 0/24, Loss: 0.48223623633384705\n",
            "Avg Loss for Epoch 9: 0.518764071787397\n",
            "Epoch 9 Metrics:\n",
            "Accuracy:  0.7874015748031497\n",
            "Precision:  0.62000124000248\n",
            "Recall:  0.7874015748031497\n",
            "F1 Score:  0.6937458808838324\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 10/20\n",
            "Step 0/24, Loss: 0.4394626319408417\n",
            "Avg Loss for Epoch 10: 0.5154539433618387\n",
            "Epoch 10 Metrics:\n",
            "Accuracy:  0.7847769028871391\n",
            "Precision:  0.6195607128056362\n",
            "Recall:  0.7847769028871391\n",
            "F1 Score:  0.6924502084298287\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 11/20\n",
            "Step 0/24, Loss: 0.6032034158706665\n",
            "Avg Loss for Epoch 11: 0.5209797980884711\n",
            "Epoch 11 Metrics:\n",
            "Accuracy:  0.7874015748031497\n",
            "Precision:  0.62000124000248\n",
            "Recall:  0.7874015748031497\n",
            "F1 Score:  0.6937458808838324\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 12/20\n",
            "Step 0/24, Loss: 0.6435678601264954\n",
            "Avg Loss for Epoch 12: 0.5119780370344719\n",
            "Epoch 12 Metrics:\n",
            "Accuracy:  0.7874015748031497\n",
            "Precision:  0.62000124000248\n",
            "Recall:  0.7874015748031497\n",
            "F1 Score:  0.6937458808838324\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 13/20\n",
            "Step 0/24, Loss: 0.337082177400589\n",
            "Avg Loss for Epoch 13: 0.5016585265596708\n",
            "Epoch 13 Metrics:\n",
            "Accuracy:  0.7874015748031497\n",
            "Precision:  0.62000124000248\n",
            "Recall:  0.7874015748031497\n",
            "F1 Score:  0.6937458808838324\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 14/20\n",
            "Step 0/24, Loss: 0.3184075653553009\n",
            "Avg Loss for Epoch 14: 0.5023121622701486\n",
            "Epoch 14 Metrics:\n",
            "Accuracy:  0.7874015748031497\n",
            "Precision:  0.62000124000248\n",
            "Recall:  0.7874015748031497\n",
            "F1 Score:  0.6937458808838324\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 15/20\n",
            "Step 0/24, Loss: 0.5731070637702942\n",
            "Avg Loss for Epoch 15: 0.486507473513484\n",
            "Epoch 15 Metrics:\n",
            "Accuracy:  0.7874015748031497\n",
            "Precision:  0.62000124000248\n",
            "Recall:  0.7874015748031497\n",
            "F1 Score:  0.6937458808838324\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 16/20\n",
            "Step 0/24, Loss: 0.3877861499786377\n",
            "Avg Loss for Epoch 16: 0.47511347010731697\n",
            "Epoch 16 Metrics:\n",
            "Accuracy:  0.7874015748031497\n",
            "Precision:  0.62000124000248\n",
            "Recall:  0.7874015748031497\n",
            "F1 Score:  0.6937458808838324\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 17/20\n",
            "Step 0/24, Loss: 0.4232398271560669\n",
            "Avg Loss for Epoch 17: 0.4600055031478405\n",
            "Epoch 17 Metrics:\n",
            "Accuracy:  0.7874015748031497\n",
            "Precision:  0.62000124000248\n",
            "Recall:  0.7874015748031497\n",
            "F1 Score:  0.6937458808838324\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 18/20\n",
            "Step 0/24, Loss: 0.4848194420337677\n",
            "Avg Loss for Epoch 18: 0.4508000798523426\n",
            "Epoch 18 Metrics:\n",
            "Accuracy:  0.7952755905511811\n",
            "Precision:  0.7792508556751262\n",
            "Recall:  0.7952755905511811\n",
            "F1 Score:  0.7204368623735898\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 19/20\n",
            "Step 0/24, Loss: 0.5382047891616821\n",
            "Avg Loss for Epoch 19: 0.43955307081341743\n",
            "Epoch 19 Metrics:\n",
            "Accuracy:  0.800524934383202\n",
            "Precision:  0.8117274110789627\n",
            "Recall:  0.800524934383202\n",
            "F1 Score:  0.7276051479537543\n",
            "Epoch completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metrics:\n",
            "Accuracy:  0.8515625\n",
            "Precision:  0.72515869140625\n",
            "Recall:  0.8515625\n",
            "F1 Score:  0.783293776371308\n",
            "Epoch 20/20\n",
            "Step 0/24, Loss: 0.23693020641803741\n",
            "Avg Loss for Epoch 20: 0.39814001756409806\n",
            "Epoch 20 Metrics:\n",
            "Accuracy:  0.8188976377952756\n",
            "Precision:  0.8288884694207744\n",
            "Recall:  0.8188976377952756\n",
            "F1 Score:  0.7670698786091783\n",
            "Epoch completed.\n",
            "Validation Metrics:\n",
            "Accuracy:  0.828125\n",
            "Precision:  0.786545868347339\n",
            "Recall:  0.828125\n",
            "F1 Score:  0.8012022243107769\n"
          ]
        }
      ],
      "id": "IGjVUpOPgFyW"
    },
    {
      "cell_type": "markdown",
      "id": "3a347ec6",
      "metadata": {
        "id": "3a347ec6"
      },
      "source": [
        "## Prediction using Best Checkpoint (epoch 20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "# Load the tokenizer and model configuration\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"/content/gdrive/MyDrive/bert_result/epoch 20\")\n",
        "\n",
        "# Assuming you already have the test DataFrame loaded as 'test'\n",
        "test_set_dataset = TheDataset(\n",
        "    reviews=test.LyricsClean.tolist(),\n",
        "    sentiments=test.sentiment.tolist(),\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_set_dataset, batch_size=1)  # Set batch_size=1 for inference\n",
        "\n",
        "# Put the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Perform prediction on the test set\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        inputs = {\n",
        "            'input_ids': batch['input_ids'],\n",
        "            'attention_mask': batch['attention_mask']\n",
        "        }\n",
        "        outputs = model(**inputs)\n",
        "        predicted_class = outputs.logits.argmax(dim=-1)\n",
        "        predictions.append(predicted_class.item())\n",
        "\n",
        "# Add the predicted labels to the test DataFrame\n",
        "test[\"predicted_sentiment\"] = predictions\n",
        "\n",
        "# Compute precision, recall, F1 score, and accuracy\n",
        "labels = test.sentiment.tolist()\n",
        "predicted_labels = test.predicted_sentiment.tolist()\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(labels, predicted_labels, average='binary')\n",
        "accuracy = accuracy_score(labels, predicted_labels)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Display the test DataFrame with the added predicted labels\n",
        "print(test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSnmaxQqdHQv",
        "outputId": "e2c7b78e-8ca3-48c0-ad1c-9bd459970a3e"
      },
      "id": "FSnmaxQqdHQv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.775\n",
            "Recall: 0.9893617021276596\n",
            "F1 Score: 0.869158878504673\n",
            "Accuracy: 0.78125\n",
            "                                                                                  lyrics  \\\n",
            "title                                                                                      \n",
            "Wait A Minute                          Why is it so hard? Youre filled with mystery I...   \n",
            "IDOL                                   You can call me artist (artist)\\nYou can call ...   \n",
            "Attack on Bangtan/The Rise of Bangtan  but what will happen if bangtan seonyeondan ri...   \n",
            "I Like It                              wanna be loveda|\\n\\ndonat wanna be fool wanna ...   \n",
            "Let me tell you right now              When it's quiet, I still look for you Sighing ...   \n",
            "...                                                                                  ...   \n",
            "So What                                somebody call me right one\\nsomebody call me w...   \n",
            "Paradise                               marathon marathon\\nlifeas long so take it slow...   \n",
            "HAPPY BIRTHDAY TO YOU                  Wherever you are, this day is for you Your hea...   \n",
            "FAKE LOVE                              For you, I could pretend like I was happy when...   \n",
            "Super Junior  Rockn Shine              Take a trip around the world Look at the magic...   \n",
            "\n",
            "                                      Language  \\\n",
            "title                                            \n",
            "Wait A Minute                               en   \n",
            "IDOL                                        en   \n",
            "Attack on Bangtan/The Rise of Bangtan       en   \n",
            "I Like It                                   en   \n",
            "Let me tell you right now                   en   \n",
            "...                                        ...   \n",
            "So What                                     en   \n",
            "Paradise                                    en   \n",
            "HAPPY BIRTHDAY TO YOU                       en   \n",
            "FAKE LOVE                                   en   \n",
            "Super Junior  Rockn Shine                   en   \n",
            "\n",
            "                                                                             rem_sp_char  \\\n",
            "title                                                                                      \n",
            "Wait A Minute                          why is it so hard youre filled with mystery i ...   \n",
            "IDOL                                   you can call me artist  you can call me idol  ...   \n",
            "Attack on Bangtan/The Rise of Bangtan  but what will happen if bangtan seonyeondan ri...   \n",
            "I Like It                              wanna be loveda  donat wanna be fool wanna be ...   \n",
            "Let me tell you right now              when its quiet i still look for you sighing fo...   \n",
            "...                                                                                  ...   \n",
            "So What                                somebody call me right one somebody call me wr...   \n",
            "Paradise                               marathon marathon lifeas long so take it slow ...   \n",
            "HAPPY BIRTHDAY TO YOU                  wherever you are this day is for you your hear...   \n",
            "FAKE LOVE                              for you i could pretend like i was happy when ...   \n",
            "Super Junior  Rockn Shine              take a trip around the world look at the magic...   \n",
            "\n",
            "                                                                             LyricsClean  \\\n",
            "title                                                                                      \n",
            "Wait A Minute                          hard filled mystery used chic feels like becom...   \n",
            "IDOL                                   call artist call idol matter call care proud f...   \n",
            "Attack on Bangtan/The Rise of Bangtan  happen bangtan seonyeondan rises bang tan nyeo...   \n",
            "I Like It                              loveda donat fool cool loved love baby want ev...   \n",
            "Let me tell you right now              quiet still look sighing reason remember witho...   \n",
            "...                                                                                  ...   \n",
            "So What                                somebody call right one somebody call wrong ia...   \n",
            "Paradise                               marathon marathon lifeas long take slow 42195 ...   \n",
            "HAPPY BIRTHDAY TO YOU                  wherever day heart making fuss long time okay ...   \n",
            "FAKE LOVE                              could pretend like happy sad could pretend lik...   \n",
            "Super Junior  Rockn Shine              take trip around world look magic made still s...   \n",
            "\n",
            "                                                                             sent_scores  \\\n",
            "title                                                                                      \n",
            "Wait A Minute                          {'neg': 0.212, 'neu': 0.568, 'pos': 0.22, 'com...   \n",
            "IDOL                                   {'neg': 0.11, 'neu': 0.56, 'pos': 0.33, 'compo...   \n",
            "Attack on Bangtan/The Rise of Bangtan  {'neg': 0.098, 'neu': 0.568, 'pos': 0.333, 'co...   \n",
            "I Like It                              {'neg': 0.166, 'neu': 0.373, 'pos': 0.46, 'com...   \n",
            "Let me tell you right now              {'neg': 0.166, 'neu': 0.572, 'pos': 0.262, 'co...   \n",
            "...                                                                                  ...   \n",
            "So What                                {'neg': 0.336, 'neu': 0.478, 'pos': 0.186, 'co...   \n",
            "Paradise                               {'neg': 0.104, 'neu': 0.503, 'pos': 0.393, 'co...   \n",
            "HAPPY BIRTHDAY TO YOU                  {'neg': 0.0, 'neu': 0.419, 'pos': 0.581, 'comp...   \n",
            "FAKE LOVE                              {'neg': 0.308, 'neu': 0.178, 'pos': 0.515, 'co...   \n",
            "Super Junior  Rockn Shine              {'neg': 0.166, 'neu': 0.563, 'pos': 0.271, 'co...   \n",
            "\n",
            "                                       comp_score  sentiment  Unnamed: 8  \\\n",
            "title                                                                      \n",
            "Wait A Minute                              0.8511          1         NaN   \n",
            "IDOL                                       0.9933          1         NaN   \n",
            "Attack on Bangtan/The Rise of Bangtan      0.9968          1         NaN   \n",
            "I Like It                                  0.9986          1         NaN   \n",
            "Let me tell you right now                  0.9575          1         NaN   \n",
            "...                                           ...        ...         ...   \n",
            "So What                                   -0.9934          0         NaN   \n",
            "Paradise                                   0.9984          1         NaN   \n",
            "HAPPY BIRTHDAY TO YOU                      0.9990          1         NaN   \n",
            "FAKE LOVE                                  0.9993          1         NaN   \n",
            "Super Junior  Rockn Shine                  0.9423          1         NaN   \n",
            "\n",
            "                                       Unnamed: 9  Unnamed: 10  Unnamed: 11  \\\n",
            "title                                                                         \n",
            "Wait A Minute                                 NaN          NaN          NaN   \n",
            "IDOL                                          NaN          NaN          NaN   \n",
            "Attack on Bangtan/The Rise of Bangtan         NaN          NaN          NaN   \n",
            "I Like It                                     NaN          NaN          NaN   \n",
            "Let me tell you right now                     NaN          NaN          NaN   \n",
            "...                                           ...          ...          ...   \n",
            "So What                                       NaN          NaN          NaN   \n",
            "Paradise                                      NaN          NaN          NaN   \n",
            "HAPPY BIRTHDAY TO YOU                         NaN          NaN          NaN   \n",
            "FAKE LOVE                                     NaN          NaN          NaN   \n",
            "Super Junior  Rockn Shine                     NaN          NaN          NaN   \n",
            "\n",
            "                                       Unnamed: 12  Unnamed: 13  vader_result  \\\n",
            "title                                                                           \n",
            "Wait A Minute                                  NaN          NaN             1   \n",
            "IDOL                                           NaN          NaN             1   \n",
            "Attack on Bangtan/The Rise of Bangtan          NaN          NaN             1   \n",
            "I Like It                                      NaN          NaN             1   \n",
            "Let me tell you right now                      NaN          NaN             1   \n",
            "...                                            ...          ...           ...   \n",
            "So What                                        NaN          NaN             0   \n",
            "Paradise                                       NaN          NaN             1   \n",
            "HAPPY BIRTHDAY TO YOU                          NaN          NaN             1   \n",
            "FAKE LOVE                                      NaN          NaN             1   \n",
            "Super Junior  Rockn Shine                      NaN          NaN             1   \n",
            "\n",
            "                                       predicted_sentiment  \n",
            "title                                                       \n",
            "Wait A Minute                                            1  \n",
            "IDOL                                                     1  \n",
            "Attack on Bangtan/The Rise of Bangtan                    1  \n",
            "I Like It                                                1  \n",
            "Let me tell you right now                                1  \n",
            "...                                                    ...  \n",
            "So What                                                  1  \n",
            "Paradise                                                 1  \n",
            "HAPPY BIRTHDAY TO YOU                                    1  \n",
            "FAKE LOVE                                                1  \n",
            "Super Junior  Rockn Shine                                1  \n",
            "\n",
            "[128 rows x 15 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install accelerate>=0.20.1\n"
      ],
      "metadata": {
        "id": "9kxzRW7BZFMY"
      },
      "id": "9kxzRW7BZFMY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers[torch]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsoEsHgFZL5y",
        "outputId": "23142678-819d-420e-c4f2-88e68bff1617"
      },
      "id": "zsoEsHgFZL5y",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.65.0)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.21.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (16.0.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}